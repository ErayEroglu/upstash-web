---
title: "Introducing Chat Completions For QStash"
slug: introducing-chat-completions-for-qstash
authors:
  - metin
tags: [announce, qstash]
---

QStash was heavily utilized to perform chat completions using large language
models as the task scheduler. It is capable of performing tasks that take
a lot of time, notify you with callbacks when it is done, or retry the task
if something goes wrong, along with other nice features.

In our pursuit of becoming a serverless data platform, we are introducing
new products and capabilities to make it easier for our users to ship
various applications faster, with prices that scales to zero.

With that in mind, we are happy to announce LLM based chat completion APIs
and seamless integration of it with all the existing QStash features!

## Chat Completions

We are starting by providing 2 models that can be used to perform multi
or single turn chat completion tasks.

With this endpoint, you will be capable of answering questions,
generating all kinds of texts, writing and debugging code,
doing translations between various languages, and all kinds of tasks
you can think of by using the state of the art open source
large language models.

```js
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
  token: "<QSTASH_TOKEN>",
});

// Get a single turn response
let response = await client.chat().create({
  provider: upstash(),
  model: "meta-llama/Meta-Llama-3-8B-Instruct",
  messages: [
    {
      role: "user",
      content: "What is the capital of Turkey?",
    },
  ],
});

console.log(response);

// Or, get a multi turn response by passing the message history
response = await client.chat().create({
  provider: upstash(),
  model: "meta-llama/Meta-Llama-3-8B-Instruct",
  messages: [
    {
      role: "user",
      content: "What is the capital of Turkey?",
    },
    {
      role: "assistant",
      content: "The capital of Turkey is Ankara.",
    },
    {
      role: "user",
      content: "What is the population of that city?",
    },
  ],
});

console.log(response);
```

To provide shorter time to first token for responsive user facing
interfaces, you can use streaming to receive chunks of chat completion
response as they become available.

```js
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
  token: "<QSTASH_TOKEN>",
});

const response = await client.chat().create({
  provider: upstash(),
  model: "meta-llama/Meta-Llama-3-8B-Instruct",
  messages: [
    {
      role: "user",
      content: "What is the capital of Turkey?",
    },
  ],
  stream: true,
});

for await (const chunk of response) {
  console.log(chunk);
}
```

Similar to the code snippets shown above, we have updated the Python SDK
to provide similar support for the chat completions, as well as supporting
all of them over REST.

This endpoint is also compatible with the OpenAI REST API. That will allow you
to use a wide variety of tools and libraries by changing just the endpoint, like
the official OpenAI Python SDK.

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://qstash.upstash.io/llm/v1",
    api_key="QSTASH_TOKEN",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Test message from the OpenAI client",
        }
    ],
    model="meta-llama/Meta-Llama-3-8B-Instruct",
)

print(chat_completion)
```

For the model inference, we are using [vLLM](https://docs.vllm.ai/en/stable/)
as a high-throughput and memory efficient engine, to deliver the maximum
performance on high-end hardware.

For the time being, we will be supporting the following models.

- `meta-llama/Meta-Llama-3-8B-Instruct`
- `mistralai/Mistral-7B-Instruct-v0.2`

We are planning to add more models with various sizes along the way, so feel
free to reach us if you are in need of a certain mode or have some model
recommendations.

See our [pricing page](https://upstash.com/pricing/qstash) for more
information about the limits and pricing of chat completions.

## Integration with QStash

The chat completions API is really useful on its own, but the important news
is that we have integrated it into existing QStash APIs seamlessly.

You can now publish or enqueue a LLM chat completion by specifying
destination as the `api/llm`.

```js
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
  token: "<QSTASH_TOKEN>",
});

await client.publishJSON({
  api: { name: "llm", provider: upstash() },
  body: {
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
      {
        role: "user",
        content: "Write me a python function that calculates the nth fibonacci number",
      },
    ],
  },
  callback: "https://your-callbak-url.com/",
});
```

This way, when the result of the chat completion request is ready, the callback
will be notified with it.

With publish and enqueue APIs, all the existing QStash features work, such as
delays, retries, ordered delivery, or controlled parallelism without an extra
effort.

```js
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
  token: "<QSTASH_TOKEN>",
});

// Publish a chat completion task that will be executed after 1 minute
// delay with at most 2 retries
await client.publishJSON({
  api: { name: "llm", provider: upstash() },
  body: {
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
      {
        role: "user",
        content: "List 10 places to go in the Mediterranean coast",
      },
    ],
  },
  callback: "https://your-callbak-url.com/",
  delay: 60,
  retries: 2,
});

// Enqueue a chat completion task with ordered delivery and
// controlled parallelism (inherited from the queue)
await client.queue({queueName: "chat-queue"}).enqueueJSON({
  api: { name: "llm", provider: upstash() },
  body: {
    model: "meta-llama/Meta-Llama-3-8B-Instruct",
    messages: [
      {
        role: "user",
        content: "List 10 places to go in the Mediterranean coast",
      },
    ],
  },
  callback: "https://your-callbak-url.com/",
});
```

Also, in case you exceed your rate limits for the chat completions, QStash
makes sure to retry your completion requests when the limit would reset, saving
you from retrying prematurely. This happens automatically in publish, enqueue, or
batch endpoints.

```js
import { Client, upstash } from "@upstash/qstash";

const client = new Client({
  token: "<QSTASH_TOKEN>",
});

# If the second request and onward fails due to rate limit error, they will be
# retried when the rate limit resets automatically.
await client.batchJSON([
  {
    api: { name: "llm", provider: upstash() },
    body: {/* chat request body */},
  },
  {
    api: { name: "llm", provider: upstash() },
    body: {/* chat request body */},
  }
])
```

Feel free to play with all of these APIs and tell us your feedback in
[Discord](https://upstash.com/discord). We would be happy to assist you
along the way, and improve the QStash with your feedback!

## Spoiler: Future of QStash

We envision QStash to be a tool to orchestrate your application logic with
various APIs that composes nicely, like a workflow.

Soon, you will be able to write code that defines what to do, and let the
QStash handle scheduling, delaying, retrying, joining, or forking
long-running jobs and let you ship your application faster and safer.

The LLM chat completion API is first of hopefully many of the APIs that will
be supported by QStash natively. We plan to bring more capabilities to QStash
soon, and let you compose all of them orthogonally with each other with type
safe code through our SDKs.
